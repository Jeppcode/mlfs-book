{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f44e17-ed2d-47e7-887d-c07490a50f83",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Colab Users - Uncomment & Run the following 2 Cells"
   ]
  },
  {
   "cell_type": "code",
   "id": "0e353e1e-ace8-4ee5-9bef-86a9155e764b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:18:31.718427Z",
     "start_time": "2025-11-07T09:18:31.709681Z"
    }
   },
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    !git clone https://github.com/featurestorebook/mlfs-book.git\n",
    "    %cd mlfs-book\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "if is_google_colab():\n",
    "    clone_repository()\n",
    "    install_dependencies()\n",
    "    root_dir = str(Path().absolute())\n",
    "    print(\"Google Colab environment\")\n",
    "else:\n",
    "    root_dir = Path().absolute()\n",
    "    # Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "    if root_dir.parts[-1:] == ('airquality',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == ('notebooks',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    root_dir = str(root_dir) \n",
    "    print(\"Local environment\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` to use the `recsys` Python module from the notebook.\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Added the following directory to the PYTHONPATH: /Users/niklasdahlbom/Documents/GitHub/mlfs-book\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "a2f3f016",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "721ae546",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:18:35.578070Z",
     "start_time": "2025-11-07T09:18:31.726217Z"
    }
   },
   "source": [
    "from xgboost import XGBRegressor\n",
    "import hopsworks\n",
    "from openai import OpenAI\n",
    "from mlfs.airquality.llm_chain import (\n",
    "    load_model, \n",
    "    get_llm_chain, \n",
    "    generate_response, \n",
    "    generate_response_openai,\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from mlfs import config\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 10:18:35,257 WARNING: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "\n",
      "2025-11-07 10:18:35,258 WARNING: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "3b062cc0",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Connect to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "id": "b6340e8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:18:37.974658Z",
     "start_time": "2025-11-07T09:18:35.582552Z"
    }
   },
   "source": [
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store() "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HopsworksSettings initialized!\n",
      "2025-11-07 10:18:35,586 INFO: Initializing external client\n",
      "2025-11-07 10:18:35,586 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 10:18:37,052 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1271998\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b6f2f191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:18:40.272643Z",
     "start_time": "2025-11-07T09:18:37.988071Z"
    }
   },
   "source": [
    "# Get_or_create the 'air_quality_fv' feature view\n",
    "feature_view = fs.get_feature_view(\n",
    "    name='air_quality_fv',\n",
    "    version=1\n",
    ")\n",
    "\n",
    "# Initialize batch scoring\n",
    "feature_view.init_batch_scoring(1)\n",
    "\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather',\n",
    "    version=1,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "8002765b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">ü™ù Retrieve AirQuality Model from Model Registry</span>"
   ]
  },
  {
   "cell_type": "code",
   "id": "02695f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:18:44.049935Z",
     "start_time": "2025-11-07T09:18:40.278004Z"
    }
   },
   "source": [
    "# Retrieve the model registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Retrieve the 'air_quality_xgboost_model' from the model registry\n",
    "retrieved_model = mr.get_model(\n",
    "    name=\"air_quality_xgboost_model\",\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "# Download the saved model artifacts  to a local directory\n",
    "saved_model_dir = retrieved_model.download()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading: 0.000%|          | 0/486550 elapsed<00:00 remaining<?"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf228ff3c69840b88e229ced2ce9ce6e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 1 files)... \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Downloading: 0.000%|          | 0/115478 elapsed<00:00 remaining<?"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffd38cd62aef446b91577a9f13de3da7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 2 files)... \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Downloading: 0.000%|          | 0/19382 elapsed<00:00 remaining<?"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8aa5e0c839a473494b75d61b25b5085"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (1 dirs, 3 files)... DONE\r"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "8930caa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:18:44.081395Z",
     "start_time": "2025-11-07T09:18:44.061060Z"
    }
   },
   "source": [
    "# Loading the XGBoost regressor model and label encoder from the saved model directory\n",
    "# model_air_quality = joblib.load(saved_model_dir + \"/xgboost_regressor.pkl\")\n",
    "model_air_quality = XGBRegressor()\n",
    "\n",
    "model_air_quality.load_model(saved_model_dir + \"/model.json\")\n",
    "\n",
    "# Displaying the retrieved XGBoost regressor model\n",
    "model_air_quality"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score='2.3885014E0', booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=['float', 'float', 'float', 'float'], gamma=None,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=&#x27;2.3885014E0&#x27;, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=[&#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;], gamma=None,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=&#x27;2.3885014E0&#x27;, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=[&#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;], gamma=None,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "fd30142d",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>‚¨áÔ∏è LLM Loading"
   ]
  },
  {
   "cell_type": "code",
   "id": "a911a86c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:20:23.254886Z",
     "start_time": "2025-11-07T09:18:50.658888Z"
    }
   },
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the LLM and its corresponding tokenizer.\n",
    "model_llm, tokenizer = load_model(model_id=\"imiraoui/OpenHermes-2.5-Mistral-7B-sharded\")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"The code execution took {duration} seconds.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1eb94e2f311411fb909d50277359527"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code execution took 92.5933358669281 seconds.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "0e329285",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>‚õìÔ∏è LangChain"
   ]
  },
  {
   "cell_type": "code",
   "id": "8caf5ffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:20:30.132245Z",
     "start_time": "2025-11-07T09:20:30.055527Z"
    }
   },
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Create and configure a language model chain.\n",
    "llm_chain = get_llm_chain(\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"The code execution took {duration} seconds.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code execution took 0.07180619239807129 seconds.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "4a2ded5c",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>üß¨ Domain-specific Evaluation Harness\n",
    "\n",
    "**Systematic evaluations** that can run automatically in CI/CD pipelines are key to evaluating models/RAG. \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "58181b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:23:29.057830Z",
     "start_time": "2025-11-07T09:20:36.133301Z"
    }
   },
   "source": [
    "QUESTION7 = \"Hi!\"\n",
    "\n",
    "response7 = generate_response(\n",
    "    QUESTION7,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response7)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m QUESTION7 = \u001B[33m\"\u001B[39m\u001B[33mHi!\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m response7 = \u001B[43mgenerate_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mQUESTION7\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_view\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweather_fg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_air_quality\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_llm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mllm_chain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(response7)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/mlfs-book/mlfs/airquality/llm_chain.py:170\u001B[39m, in \u001B[36mgenerate_response\u001B[39m\u001B[34m(user_query, feature_view, weather_fg, model_air_quality, model_llm, tokenizer, llm_chain, verbose)\u001B[39m\n\u001B[32m    154\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    155\u001B[39m \u001B[33;03mGenerate response to user query using LLM chain and context data.\u001B[39;00m\n\u001B[32m    156\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    167\u001B[39m \u001B[33;03m    str: Generated response to the user query.\u001B[39;00m\n\u001B[32m    168\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    169\u001B[39m \u001B[38;5;66;03m# Get context data based on user query\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m170\u001B[39m context = \u001B[43mget_context_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[43m    \u001B[49m\u001B[43muser_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    172\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_view\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweather_fg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_air_quality\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_llm\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_llm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    177\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[38;5;66;03m# Get today's date in a readable format\u001B[39;00m\n\u001B[32m    180\u001B[39m date_today = \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdatetime.date.today().strftime(\u001B[33m\"\u001B[39m\u001B[33m%\u001B[39m\u001B[33mA\u001B[39m\u001B[33m\"\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdatetime.date.today()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/mlfs-book/mlfs/airquality/context_engineering.py:225\u001B[39m, in \u001B[36mget_context_data\u001B[39m\u001B[34m(user_query, feature_view, weather_fg, model_air_quality, model_llm, tokenizer, client)\u001B[39m\n\u001B[32m    221\u001B[39m     completion = function_calling_with_openai(user_query, client)\n\u001B[32m    223\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    224\u001B[39m     \u001B[38;5;66;03m# Generate a response using LLM\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m225\u001B[39m     completion = \u001B[43mgenerate_hermes\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[43m        \u001B[49m\u001B[43muser_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    227\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_llm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    228\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    229\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    231\u001B[39m \u001B[38;5;66;03m# Extract function calls from the completion\u001B[39;00m\n\u001B[32m    232\u001B[39m functions = extract_function_calls(completion)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/mlfs-book/mlfs/airquality/context_engineering.py:121\u001B[39m, in \u001B[36mgenerate_hermes\u001B[39m\u001B[34m(user_query, model_llm, tokenizer)\u001B[39m\n\u001B[32m    119\u001B[39m input_size = tokens.input_ids.numel()\n\u001B[32m    120\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.inference_mode():\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m     generated_tokens = \u001B[43mmodel_llm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    122\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    123\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    124\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    125\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    126\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    127\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m512\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer.decode(\n\u001B[32m    134\u001B[39m     generated_tokens.squeeze()[input_size:],\n\u001B[32m    135\u001B[39m     skip_special_tokens=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    136\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/generation/utils.py:1592\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[39m\n\u001B[32m   1584\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   1585\u001B[39m         input_ids=input_ids,\n\u001B[32m   1586\u001B[39m         expand_size=generation_config.num_return_sequences,\n\u001B[32m   1587\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   1588\u001B[39m         **model_kwargs,\n\u001B[32m   1589\u001B[39m     )\n\u001B[32m   1591\u001B[39m     \u001B[38;5;66;03m# 13. run sample\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1592\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1593\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1594\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1595\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_warper\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlogits_warper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1596\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1597\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1598\u001B[39m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1599\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1600\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_logits\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43moutput_logits\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1601\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1602\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1603\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1604\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1605\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1607\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode == GenerationMode.BEAM_SEARCH:\n\u001B[32m   1608\u001B[39m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[32m   1609\u001B[39m     beam_scorer = BeamSearchScorer(\n\u001B[32m   1610\u001B[39m         batch_size=batch_size,\n\u001B[32m   1611\u001B[39m         num_beams=generation_config.num_beams,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1616\u001B[39m         max_length=generation_config.max_length,\n\u001B[32m   1617\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/generation/utils.py:2696\u001B[39m, in \u001B[36mGenerationMixin.sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   2693\u001B[39m model_inputs = \u001B[38;5;28mself\u001B[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001B[32m   2695\u001B[39m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2696\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m   2697\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2698\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   2699\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2700\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2701\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2703\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[32m   2704\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001B[39m, in \u001B[36mMistralForCausalLM.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1154\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m   1156\u001B[39m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1157\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1158\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1159\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1160\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1161\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1162\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1163\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1164\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1165\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1166\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1167\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1169\u001B[39m hidden_states = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1170\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.lm_head(hidden_states)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1042\u001B[39m, in \u001B[36mMistralModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1032\u001B[39m     layer_outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m   1033\u001B[39m         decoder_layer.\u001B[34m__call__\u001B[39m,\n\u001B[32m   1034\u001B[39m         hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1039\u001B[39m         use_cache,\n\u001B[32m   1040\u001B[39m     )\n\u001B[32m   1041\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1042\u001B[39m     layer_outputs = \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1043\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1044\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1045\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1046\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1047\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1048\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1049\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1051\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1053\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:770\u001B[39m, in \u001B[36mMistralDecoderLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001B[39m\n\u001B[32m    768\u001B[39m residual = hidden_states\n\u001B[32m    769\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.post_attention_layernorm(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m770\u001B[39m hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    771\u001B[39m hidden_states = residual + hidden_states\n\u001B[32m    773\u001B[39m outputs = (hidden_states,)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:179\u001B[39m, in \u001B[36mMistralMLP.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    178\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.down_proj(\u001B[38;5;28mself\u001B[39m.act_fn(\u001B[38;5;28mself\u001B[39m.gate_proj(x)) * \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mup_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/linear.py:134\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m    131\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m    Runs the forward pass.\u001B[39;00m\n\u001B[32m    133\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "4ec32e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:31:45.513053Z",
     "start_time": "2025-11-07T09:23:31.059294Z"
    }
   },
   "source": [
    "QUESTION = \"Who are you?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m QUESTION = \u001B[33m\"\u001B[39m\u001B[33mWho are you?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m response = \u001B[43mgenerate_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mQUESTION\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_view\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweather_fg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_air_quality\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_llm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mllm_chain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/mlfs-book/mlfs/airquality/llm_chain.py:170\u001B[39m, in \u001B[36mgenerate_response\u001B[39m\u001B[34m(user_query, feature_view, weather_fg, model_air_quality, model_llm, tokenizer, llm_chain, verbose)\u001B[39m\n\u001B[32m    154\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    155\u001B[39m \u001B[33;03mGenerate response to user query using LLM chain and context data.\u001B[39;00m\n\u001B[32m    156\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    167\u001B[39m \u001B[33;03m    str: Generated response to the user query.\u001B[39;00m\n\u001B[32m    168\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    169\u001B[39m \u001B[38;5;66;03m# Get context data based on user query\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m170\u001B[39m context = \u001B[43mget_context_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[43m    \u001B[49m\u001B[43muser_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    172\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_view\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweather_fg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_air_quality\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_llm\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_llm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    177\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[38;5;66;03m# Get today's date in a readable format\u001B[39;00m\n\u001B[32m    180\u001B[39m date_today = \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdatetime.date.today().strftime(\u001B[33m\"\u001B[39m\u001B[33m%\u001B[39m\u001B[33mA\u001B[39m\u001B[33m\"\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdatetime.date.today()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/mlfs-book/mlfs/airquality/context_engineering.py:225\u001B[39m, in \u001B[36mget_context_data\u001B[39m\u001B[34m(user_query, feature_view, weather_fg, model_air_quality, model_llm, tokenizer, client)\u001B[39m\n\u001B[32m    221\u001B[39m     completion = function_calling_with_openai(user_query, client)\n\u001B[32m    223\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    224\u001B[39m     \u001B[38;5;66;03m# Generate a response using LLM\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m225\u001B[39m     completion = \u001B[43mgenerate_hermes\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[43m        \u001B[49m\u001B[43muser_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    227\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_llm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    228\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    229\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    231\u001B[39m \u001B[38;5;66;03m# Extract function calls from the completion\u001B[39;00m\n\u001B[32m    232\u001B[39m functions = extract_function_calls(completion)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/mlfs-book/mlfs/airquality/context_engineering.py:121\u001B[39m, in \u001B[36mgenerate_hermes\u001B[39m\u001B[34m(user_query, model_llm, tokenizer)\u001B[39m\n\u001B[32m    119\u001B[39m input_size = tokens.input_ids.numel()\n\u001B[32m    120\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.inference_mode():\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m     generated_tokens = \u001B[43mmodel_llm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    122\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    123\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    124\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    125\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    126\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    127\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m512\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer.decode(\n\u001B[32m    134\u001B[39m     generated_tokens.squeeze()[input_size:],\n\u001B[32m    135\u001B[39m     skip_special_tokens=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    136\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/generation/utils.py:1592\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[39m\n\u001B[32m   1584\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   1585\u001B[39m         input_ids=input_ids,\n\u001B[32m   1586\u001B[39m         expand_size=generation_config.num_return_sequences,\n\u001B[32m   1587\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   1588\u001B[39m         **model_kwargs,\n\u001B[32m   1589\u001B[39m     )\n\u001B[32m   1591\u001B[39m     \u001B[38;5;66;03m# 13. run sample\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1592\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1593\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1594\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1595\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_warper\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlogits_warper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1596\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1597\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1598\u001B[39m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1599\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1600\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_logits\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43moutput_logits\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1601\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1602\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1603\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1604\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1605\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1607\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode == GenerationMode.BEAM_SEARCH:\n\u001B[32m   1608\u001B[39m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[32m   1609\u001B[39m     beam_scorer = BeamSearchScorer(\n\u001B[32m   1610\u001B[39m         batch_size=batch_size,\n\u001B[32m   1611\u001B[39m         num_beams=generation_config.num_beams,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1616\u001B[39m         max_length=generation_config.max_length,\n\u001B[32m   1617\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/generation/utils.py:2696\u001B[39m, in \u001B[36mGenerationMixin.sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   2693\u001B[39m model_inputs = \u001B[38;5;28mself\u001B[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001B[32m   2695\u001B[39m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2696\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m   2697\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2698\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   2699\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2700\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2701\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2703\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[32m   2704\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001B[39m, in \u001B[36mMistralForCausalLM.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1154\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m   1156\u001B[39m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1157\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1158\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1159\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1160\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1161\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1162\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1163\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1164\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1165\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1166\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1167\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1169\u001B[39m hidden_states = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1170\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.lm_head(hidden_states)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1042\u001B[39m, in \u001B[36mMistralModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1032\u001B[39m     layer_outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m   1033\u001B[39m         decoder_layer.\u001B[34m__call__\u001B[39m,\n\u001B[32m   1034\u001B[39m         hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1039\u001B[39m         use_cache,\n\u001B[32m   1040\u001B[39m     )\n\u001B[32m   1041\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1042\u001B[39m     layer_outputs = \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1043\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1044\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1045\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1046\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1047\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1048\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1049\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1051\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1053\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:770\u001B[39m, in \u001B[36mMistralDecoderLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001B[39m\n\u001B[32m    768\u001B[39m residual = hidden_states\n\u001B[32m    769\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.post_attention_layernorm(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m770\u001B[39m hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    771\u001B[39m hidden_states = residual + hidden_states\n\u001B[32m    773\u001B[39m outputs = (hidden_states,)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:179\u001B[39m, in \u001B[36mMistralMLP.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    178\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdown_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mact_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgate_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mup_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/ml-lab-py311/lib/python3.11/site-packages/torch/nn/modules/linear.py:134\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m    131\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m    Runs the forward pass.\u001B[39;00m\n\u001B[32m    133\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION1 = \"What was the average air quality from 2024-01-10 till 2024-01-14?\"\n",
    "\n",
    "response1 = generate_response(\n",
    "    QUESTION1, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d01dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION11 = \"When and what was the air quality like last week?\"\n",
    "\n",
    "response11 = generate_response(\n",
    "    QUESTION11, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION12 = \"When and what was the minimum air quality from 2024-01-10 till 2024-01-14?\"\n",
    "\n",
    "response12 = generate_response(\n",
    "    QUESTION12, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION2a = \"What was the air quality like last week?\"\n",
    "\n",
    "response2 = generate_response(\n",
    "    QUESTION2a,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION2 = \"What was the air quality like yesterday?\"\n",
    "\n",
    "response2 = generate_response(\n",
    "    QUESTION2,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed349483",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION3 = \"What will the air quality be like next Tuesday?\"\n",
    "\n",
    "response3 = generate_response(\n",
    "    QUESTION3, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6825c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION4 = \"What will the air quality be like the day after tomorrow?\"\n",
    "\n",
    "response4 = generate_response(\n",
    "    QUESTION4, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION5 = \"What will the air quality be like this Sunday?\"\n",
    "\n",
    "response5 = generate_response(\n",
    "    QUESTION5, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee271416",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION7 = \"What will the air quality be like for the rest of the week?\"\n",
    "\n",
    "response7 = generate_response(\n",
    "    QUESTION7, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeeb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Will the air quality be safe or not for the next week?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION7, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Is tomorrow's air quality level dangerous?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb26726",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Can you please explain different PM2_5 air quality levels?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c2a463f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:31:54.151855Z",
     "start_time": "2025-11-07T09:31:54.146186Z"
    }
   },
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "09fb77d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:32:12.290455Z",
     "start_time": "2025-11-07T09:31:55.494679Z"
    }
   },
   "source": [
    "#!pip install openai --quiet\n",
    "#!pip install gradio==3.40.1 --quiet"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "9f4aebe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T09:32:21.902349Z",
     "start_time": "2025-11-07T09:32:21.834669Z"
    }
   },
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from mlfs.airquality.llm_chain import load_model, get_llm_chain, generate_response\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgradio\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgr\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pipeline\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'gradio'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ASR pipeline\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "def transcribe(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    if y.ndim > 1 and y.shape[1] > 1:\n",
    "        y = np.mean(y, axis=1)\n",
    "    y /= np.max(np.abs(y))\n",
    "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
    "\n",
    "def generate_query_response(user_query, method, openai_api_key=None):\n",
    "    if method == 'Hermes LLM':        \n",
    "        response = generate_response(\n",
    "            user_query,\n",
    "            feature_view,\n",
    "            weather_fg,\n",
    "            model_air_quality,\n",
    "            model_llm,\n",
    "            tokenizer,\n",
    "            llm_chain,\n",
    "            verbose=False,\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    elif method == 'OpenAI API' and openai_api_key:\n",
    "        client = OpenAI(\n",
    "            api_key=openai_api_key\n",
    "        )\n",
    "        \n",
    "        response = generate_response_openai(   \n",
    "            user_query,\n",
    "            feature_view,\n",
    "            weather_fg,\n",
    "            model_air_quality,\n",
    "            client=client,\n",
    "            verbose=True,\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    else:\n",
    "        return \"Invalid method or missing API key.\"\n",
    "\n",
    "def handle_input(text_input=None, audio_input=None, method='Hermes LLM', openai_api_key=\"\"):\n",
    "    if audio_input is not None:\n",
    "        user_query = transcribe(audio_input)\n",
    "    else:\n",
    "        user_query = text_input\n",
    "    \n",
    "    # Check if OpenAI API key is required but not provided\n",
    "    if method == 'OpenAI API' and not openai_api_key.strip():\n",
    "        return \"OpenAI API key is required for this method.\"\n",
    "\n",
    "    if user_query:\n",
    "        return generate_query_response(user_query, method, openai_api_key)\n",
    "    else:\n",
    "        return \"Please provide input either via text or voice.\"\n",
    "    \n",
    "\n",
    "# Setting up the Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=handle_input,\n",
    "    inputs=[\n",
    "        gr.Textbox(placeholder=\"Type here or use voice input...\"), \n",
    "        gr.Audio(), \n",
    "        gr.Radio([\"Hermes LLM\", \"OpenAI API\"], label=\"Choose the response generation method\"),\n",
    "        gr.Textbox(label=\"Enter your OpenAI API key (only if you selected OpenAI API):\", type=\"password\")  # Removed `optional=True`\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"üå§Ô∏è AirQuality AI Assistant üí¨\",\n",
    "    description=\"Ask your questions about air quality or use your voice to interact. Select the response generation method and provide an OpenAI API key if necessary.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa7c6a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (ml-lab)",
   "language": "python",
   "name": "ml-lab-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
